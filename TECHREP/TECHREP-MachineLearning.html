<!doctype html>
<html>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>
<title>TECHREP-MachineLearning</title><style type='text/css'>html {overflow-x: initial !important;}:root { --bg-color:#ffffff; --text-color:#333333; --select-text-bg-color:#B5D6FC; --select-text-font-color:auto; --monospace:"Lucida Console",Consolas,"Courier",monospace; }
html { font-size: 14px; background-color: var(--bg-color); color: var(--text-color); font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; -webkit-font-smoothing: antialiased; }
body { margin: 0px; padding: 0px; height: auto; bottom: 0px; top: 0px; left: 0px; right: 0px; font-size: 1rem; line-height: 1.42857; overflow-x: hidden; background: inherit; tab-size: 4; }
iframe { margin: auto; }
a.url { word-break: break-all; }
a:active, a:hover { outline: 0px; }
.in-text-selection, ::selection { text-shadow: none; background: var(--select-text-bg-color); color: var(--select-text-font-color); }
#write { margin: 0px auto; height: auto; width: inherit; word-break: normal; overflow-wrap: break-word; position: relative; white-space: normal; overflow-x: visible; padding-top: 40px; }
#write.first-line-indent p { text-indent: 2em; }
#write.first-line-indent li p, #write.first-line-indent p * { text-indent: 0px; }
#write.first-line-indent li { margin-left: 2em; }
.for-image #write { padding-left: 8px; padding-right: 8px; }
body.typora-export { padding-left: 30px; padding-right: 30px; }
.typora-export .footnote-line, .typora-export li, .typora-export p { white-space: pre-wrap; }
@media screen and (max-width: 500px) {
  body.typora-export { padding-left: 0px; padding-right: 0px; }
  #write { padding-left: 20px; padding-right: 20px; }
  .CodeMirror-sizer { margin-left: 0px !important; }
  .CodeMirror-gutters { display: none !important; }
}
#write li > figure:last-child { margin-bottom: 0.5rem; }
#write ol, #write ul { position: relative; }
img { max-width: 100%; vertical-align: middle; }
button, input, select, textarea { color: inherit; font: inherit; }
input[type="checkbox"], input[type="radio"] { line-height: normal; padding: 0px; }
*, ::after, ::before { box-sizing: border-box; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p, #write pre { width: inherit; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p { position: relative; }
p { line-height: inherit; }
h1, h2, h3, h4, h5, h6 { break-after: avoid-page; break-inside: avoid; orphans: 2; }
p { orphans: 4; }
h1 { font-size: 2rem; }
h2 { font-size: 1.8rem; }
h3 { font-size: 1.6rem; }
h4 { font-size: 1.4rem; }
h5 { font-size: 1.2rem; }
h6 { font-size: 1rem; }
.md-math-block, .md-rawblock, h1, h2, h3, h4, h5, h6, p { margin-top: 1rem; margin-bottom: 1rem; }
.hidden { display: none; }
.md-blockmeta { color: rgb(204, 204, 204); font-weight: 700; font-style: italic; }
a { cursor: pointer; }
sup.md-footnote { padding: 2px 4px; background-color: rgba(238, 238, 238, 0.7); color: rgb(85, 85, 85); border-radius: 4px; cursor: pointer; }
sup.md-footnote a, sup.md-footnote a:hover { color: inherit; text-transform: inherit; text-decoration: inherit; }
#write input[type="checkbox"] { cursor: pointer; width: inherit; height: inherit; }
figure { overflow-x: auto; margin: 1.2em 0px; max-width: calc(100% + 16px); padding: 0px; }
figure > table { margin: 0px !important; }
tr { break-inside: avoid; break-after: auto; }
thead { display: table-header-group; }
table { border-collapse: collapse; border-spacing: 0px; width: 100%; overflow: auto; break-inside: auto; text-align: left; }
table.md-table td { min-width: 32px; }
.CodeMirror-gutters { border-right: 0px; background-color: inherit; }
.CodeMirror-linenumber { user-select: none; }
.CodeMirror { text-align: left; }
.CodeMirror-placeholder { opacity: 0.3; }
.CodeMirror pre { padding: 0px 4px; }
.CodeMirror-lines { padding: 0px; }
div.hr:focus { cursor: none; }
#write pre { white-space: pre-wrap; }
#write.fences-no-line-wrapping pre { white-space: pre; }
#write pre.ty-contain-cm { white-space: normal; }
.CodeMirror-gutters { margin-right: 4px; }
.md-fences { font-size: 0.9rem; display: block; break-inside: avoid; text-align: left; overflow: visible; white-space: pre; background: inherit; position: relative !important; }
.md-diagram-panel { width: 100%; margin-top: 10px; text-align: center; padding-top: 0px; padding-bottom: 8px; overflow-x: auto; }
#write .md-fences.mock-cm { white-space: pre-wrap; }
.md-fences.md-fences-with-lineno { padding-left: 0px; }
#write.fences-no-line-wrapping .md-fences.mock-cm { white-space: pre; overflow-x: auto; }
.md-fences.mock-cm.md-fences-with-lineno { padding-left: 8px; }
.CodeMirror-line, twitterwidget { break-inside: avoid; }
.footnotes { opacity: 0.8; font-size: 0.9rem; margin-top: 1em; margin-bottom: 1em; }
.footnotes + .footnotes { margin-top: 0px; }
.md-reset { margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: top; background: 0px 0px; text-decoration: none; text-shadow: none; float: none; position: static; width: auto; height: auto; white-space: nowrap; cursor: inherit; -webkit-tap-highlight-color: transparent; line-height: normal; font-weight: 400; text-align: left; box-sizing: content-box; direction: ltr; }
li div { padding-top: 0px; }
blockquote { margin: 1rem 0px; }
li .mathjax-block, li p { margin: 0.5rem 0px; }
li { margin: 0px; position: relative; }
blockquote > :last-child { margin-bottom: 0px; }
blockquote > :first-child, li > :first-child { margin-top: 0px; }
.footnotes-area { color: rgb(136, 136, 136); margin-top: 0.714rem; padding-bottom: 0.143rem; white-space: normal; }
#write .footnote-line { white-space: pre-wrap; }
@media print {
  body, html { border: 1px solid transparent; height: 99%; break-after: avoid; break-before: avoid; }
  #write { margin-top: 0px; padding-top: 0px; border-color: transparent !important; }
  .typora-export * { -webkit-print-color-adjust: exact; }
  html.blink-to-pdf { font-size: 13px; }
  .typora-export #write { padding-left: 32px; padding-right: 32px; padding-bottom: 0px; break-after: avoid; }
  .typora-export #write::after { height: 0px; }
}
.footnote-line { margin-top: 0.714em; font-size: 0.7em; }
a img, img a { cursor: pointer; }
pre.md-meta-block { font-size: 0.8rem; min-height: 0.8rem; white-space: pre-wrap; background: rgb(204, 204, 204); display: block; overflow-x: hidden; }
p > .md-image:only-child:not(.md-img-error) img, p > img:only-child { display: block; margin: auto; }
p > .md-image:only-child { display: inline-block; width: 100%; }
#write .MathJax_Display { margin: 0.8em 0px 0px; }
.md-math-block { width: 100%; }
.md-math-block:not(:empty)::after { display: none; }
[contenteditable="true"]:active, [contenteditable="true"]:focus { outline: 0px; box-shadow: none; }
.md-task-list-item { position: relative; list-style-type: none; }
.task-list-item.md-task-list-item { padding-left: 0px; }
.md-task-list-item > input { position: absolute; top: 0px; left: 0px; margin-left: -1.2em; margin-top: calc(1em - 10px); border: none; }
.math { font-size: 1rem; }
.md-toc { min-height: 3.58rem; position: relative; font-size: 0.9rem; border-radius: 10px; }
.md-toc-content { position: relative; margin-left: 0px; }
.md-toc-content::after, .md-toc::after { display: none; }
.md-toc-item { display: block; color: rgb(65, 131, 196); }
.md-toc-item a { text-decoration: none; }
.md-toc-inner:hover { text-decoration: underline; }
.md-toc-inner { display: inline-block; cursor: pointer; }
.md-toc-h1 .md-toc-inner { margin-left: 0px; font-weight: 700; }
.md-toc-h2 .md-toc-inner { margin-left: 2em; }
.md-toc-h3 .md-toc-inner { margin-left: 4em; }
.md-toc-h4 .md-toc-inner { margin-left: 6em; }
.md-toc-h5 .md-toc-inner { margin-left: 8em; }
.md-toc-h6 .md-toc-inner { margin-left: 10em; }
@media screen and (max-width: 48em) {
  .md-toc-h3 .md-toc-inner { margin-left: 3.5em; }
  .md-toc-h4 .md-toc-inner { margin-left: 5em; }
  .md-toc-h5 .md-toc-inner { margin-left: 6.5em; }
  .md-toc-h6 .md-toc-inner { margin-left: 8em; }
}
a.md-toc-inner { font-size: inherit; font-style: inherit; font-weight: inherit; line-height: inherit; }
.footnote-line a:not(.reversefootnote) { color: inherit; }
.md-attr { display: none; }
.md-fn-count::after { content: "."; }
code, pre, samp, tt { font-family: var(--monospace); }
kbd { margin: 0px 0.1em; padding: 0.1em 0.6em; font-size: 0.8em; color: rgb(36, 39, 41); background: rgb(255, 255, 255); border: 1px solid rgb(173, 179, 185); border-radius: 3px; box-shadow: rgba(12, 13, 14, 0.2) 0px 1px 0px, rgb(255, 255, 255) 0px 0px 0px 2px inset; white-space: nowrap; vertical-align: middle; }
.md-comment { color: rgb(162, 127, 3); opacity: 0.8; font-family: var(--monospace); }
code { text-align: left; vertical-align: initial; }
a.md-print-anchor { white-space: pre !important; border-width: initial !important; border-style: none !important; border-color: initial !important; display: inline-block !important; position: absolute !important; width: 1px !important; right: 0px !important; outline: 0px !important; background: 0px 0px !important; text-decoration: initial !important; text-shadow: initial !important; }
.md-inline-math .MathJax_SVG .noError { display: none !important; }
.html-for-mac .inline-math-svg .MathJax_SVG { vertical-align: 0.2px; }
.md-math-block .MathJax_SVG_Display { text-align: center; margin: 0px; position: relative; text-indent: 0px; max-width: none; max-height: none; min-height: 0px; min-width: 100%; width: auto; overflow-y: hidden; display: block !important; }
.MathJax_SVG_Display, .md-inline-math .MathJax_SVG_Display { width: auto; margin: inherit; display: inline-block !important; }
.MathJax_SVG .MJX-monospace { font-family: var(--monospace); }
.MathJax_SVG .MJX-sans-serif { font-family: sans-serif; }
.MathJax_SVG { display: inline; font-style: normal; font-weight: 400; line-height: normal; zoom: 90%; text-indent: 0px; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; padding: 0px; margin: 0px; }
.MathJax_SVG * { transition: none 0s ease 0s; }
.MathJax_SVG_Display svg { vertical-align: middle !important; margin-bottom: 0px !important; margin-top: 0px !important; }
.os-windows.monocolor-emoji .md-emoji { font-family: "Segoe UI Symbol", sans-serif; }
.md-diagram-panel > svg { max-width: 100%; }
[lang="mermaid"] svg, [lang="flow"] svg { max-width: 100%; height: auto; }
[lang="mermaid"] .node text { font-size: 1rem; }
table tr th { border-bottom: 0px; }
video { max-width: 100%; display: block; margin: 0px auto; }
iframe { max-width: 100%; width: 100%; border: none; }
.highlight td, .highlight tr { border: 0px; }


.CodeMirror { height: auto; }
.CodeMirror.cm-s-inner { background: inherit; }
.CodeMirror-scroll { overflow: auto hidden; z-index: 3; }
.CodeMirror-gutter-filler, .CodeMirror-scrollbar-filler { background-color: rgb(255, 255, 255); }
.CodeMirror-gutters { border-right: 1px solid rgb(221, 221, 221); background: inherit; white-space: nowrap; }
.CodeMirror-linenumber { padding: 0px 3px 0px 5px; text-align: right; color: rgb(153, 153, 153); }
.cm-s-inner .cm-keyword { color: rgb(119, 0, 136); }
.cm-s-inner .cm-atom, .cm-s-inner.cm-atom { color: rgb(34, 17, 153); }
.cm-s-inner .cm-number { color: rgb(17, 102, 68); }
.cm-s-inner .cm-def { color: rgb(0, 0, 255); }
.cm-s-inner .cm-variable { color: rgb(0, 0, 0); }
.cm-s-inner .cm-variable-2 { color: rgb(0, 85, 170); }
.cm-s-inner .cm-variable-3 { color: rgb(0, 136, 85); }
.cm-s-inner .cm-string { color: rgb(170, 17, 17); }
.cm-s-inner .cm-property { color: rgb(0, 0, 0); }
.cm-s-inner .cm-operator { color: rgb(152, 26, 26); }
.cm-s-inner .cm-comment, .cm-s-inner.cm-comment { color: rgb(170, 85, 0); }
.cm-s-inner .cm-string-2 { color: rgb(255, 85, 0); }
.cm-s-inner .cm-meta { color: rgb(85, 85, 85); }
.cm-s-inner .cm-qualifier { color: rgb(85, 85, 85); }
.cm-s-inner .cm-builtin { color: rgb(51, 0, 170); }
.cm-s-inner .cm-bracket { color: rgb(153, 153, 119); }
.cm-s-inner .cm-tag { color: rgb(17, 119, 0); }
.cm-s-inner .cm-attribute { color: rgb(0, 0, 204); }
.cm-s-inner .cm-header, .cm-s-inner.cm-header { color: rgb(0, 0, 255); }
.cm-s-inner .cm-quote, .cm-s-inner.cm-quote { color: rgb(0, 153, 0); }
.cm-s-inner .cm-hr, .cm-s-inner.cm-hr { color: rgb(153, 153, 153); }
.cm-s-inner .cm-link, .cm-s-inner.cm-link { color: rgb(0, 0, 204); }
.cm-negative { color: rgb(221, 68, 68); }
.cm-positive { color: rgb(34, 153, 34); }
.cm-header, .cm-strong { font-weight: 700; }
.cm-del { text-decoration: line-through; }
.cm-em { font-style: italic; }
.cm-link { text-decoration: underline; }
.cm-error { color: red; }
.cm-invalidchar { color: red; }
.cm-constant { color: rgb(38, 139, 210); }
.cm-defined { color: rgb(181, 137, 0); }
div.CodeMirror span.CodeMirror-matchingbracket { color: rgb(0, 255, 0); }
div.CodeMirror span.CodeMirror-nonmatchingbracket { color: rgb(255, 34, 34); }
.cm-s-inner .CodeMirror-activeline-background { background: inherit; }
.CodeMirror { position: relative; overflow: hidden; }
.CodeMirror-scroll { height: 100%; outline: 0px; position: relative; box-sizing: content-box; background: inherit; }
.CodeMirror-sizer { position: relative; }
.CodeMirror-gutter-filler, .CodeMirror-hscrollbar, .CodeMirror-scrollbar-filler, .CodeMirror-vscrollbar { position: absolute; z-index: 6; display: none; }
.CodeMirror-vscrollbar { right: 0px; top: 0px; overflow: hidden; }
.CodeMirror-hscrollbar { bottom: 0px; left: 0px; overflow: hidden; }
.CodeMirror-scrollbar-filler { right: 0px; bottom: 0px; }
.CodeMirror-gutter-filler { left: 0px; bottom: 0px; }
.CodeMirror-gutters { position: absolute; left: 0px; top: 0px; padding-bottom: 30px; z-index: 3; }
.CodeMirror-gutter { white-space: normal; height: 100%; box-sizing: content-box; padding-bottom: 30px; margin-bottom: -32px; display: inline-block; }
.CodeMirror-gutter-wrapper { position: absolute; z-index: 4; background: 0px 0px !important; border: none !important; }
.CodeMirror-gutter-background { position: absolute; top: 0px; bottom: 0px; z-index: 4; }
.CodeMirror-gutter-elt { position: absolute; cursor: default; z-index: 4; }
.CodeMirror-lines { cursor: text; }
.CodeMirror pre { border-radius: 0px; border-width: 0px; background: 0px 0px; font-family: inherit; font-size: inherit; margin: 0px; white-space: pre; overflow-wrap: normal; color: inherit; z-index: 2; position: relative; overflow: visible; }
.CodeMirror-wrap pre { overflow-wrap: break-word; white-space: pre-wrap; word-break: normal; }
.CodeMirror-code pre { border-right: 30px solid transparent; width: fit-content; }
.CodeMirror-wrap .CodeMirror-code pre { border-right: none; width: auto; }
.CodeMirror-linebackground { position: absolute; left: 0px; right: 0px; top: 0px; bottom: 0px; z-index: 0; }
.CodeMirror-linewidget { position: relative; z-index: 2; overflow: auto; }
.CodeMirror-wrap .CodeMirror-scroll { overflow-x: hidden; }
.CodeMirror-measure { position: absolute; width: 100%; height: 0px; overflow: hidden; visibility: hidden; }
.CodeMirror-measure pre { position: static; }
.CodeMirror div.CodeMirror-cursor { position: absolute; visibility: hidden; border-right: none; width: 0px; }
.CodeMirror div.CodeMirror-cursor { visibility: hidden; }
.CodeMirror-focused div.CodeMirror-cursor { visibility: inherit; }
.cm-searching { background: rgba(255, 255, 0, 0.4); }
@media print {
  .CodeMirror div.CodeMirror-cursor { visibility: hidden; }
}


:root { --side-bar-bg-color: #fafafa; --control-text-color: #777; }
html { font-size: 16px; }
body { font-family: "Open Sans", "Clear Sans", "Helvetica Neue", Helvetica, Arial, sans-serif; color: rgb(51, 51, 51); line-height: 1.6; }
#write { max-width: 860px; margin: 0px auto; padding: 30px 30px 100px; }
#write > ul:first-child, #write > ol:first-child { margin-top: 30px; }
a { color: rgb(65, 131, 196); }
h1, h2, h3, h4, h5, h6 { position: relative; margin-top: 1rem; margin-bottom: 1rem; font-weight: bold; line-height: 1.4; cursor: text; }
h1:hover a.anchor, h2:hover a.anchor, h3:hover a.anchor, h4:hover a.anchor, h5:hover a.anchor, h6:hover a.anchor { text-decoration: none; }
h1 tt, h1 code { font-size: inherit; }
h2 tt, h2 code { font-size: inherit; }
h3 tt, h3 code { font-size: inherit; }
h4 tt, h4 code { font-size: inherit; }
h5 tt, h5 code { font-size: inherit; }
h6 tt, h6 code { font-size: inherit; }
h1 { padding-bottom: 0.3em; font-size: 2.25em; line-height: 1.2; border-bottom: 1px solid rgb(238, 238, 238); }
h2 { padding-bottom: 0.3em; font-size: 1.75em; line-height: 1.225; border-bottom: 1px solid rgb(238, 238, 238); }
h3 { font-size: 1.5em; line-height: 1.43; }
h4 { font-size: 1.25em; }
h5 { font-size: 1em; }
h6 { font-size: 1em; color: rgb(119, 119, 119); }
p, blockquote, ul, ol, dl, table { margin: 0.8em 0px; }
li > ol, li > ul { margin: 0px; }
hr { height: 2px; padding: 0px; margin: 16px 0px; background-color: rgb(231, 231, 231); border: 0px none; overflow: hidden; box-sizing: content-box; }
li p.first { display: inline-block; }
ul, ol { padding-left: 30px; }
ul:first-child, ol:first-child { margin-top: 0px; }
ul:last-child, ol:last-child { margin-bottom: 0px; }
blockquote { border-left: 4px solid rgb(223, 226, 229); padding: 0px 15px; color: rgb(119, 119, 119); }
blockquote blockquote { padding-right: 0px; }
table { padding: 0px; word-break: initial; }
table tr { border-top: 1px solid rgb(223, 226, 229); margin: 0px; padding: 0px; }
table tr:nth-child(2n), thead { background-color: rgb(248, 248, 248); }
table tr th { font-weight: bold; border-width: 1px 1px 0px; border-top-style: solid; border-right-style: solid; border-left-style: solid; border-top-color: rgb(223, 226, 229); border-right-color: rgb(223, 226, 229); border-left-color: rgb(223, 226, 229); border-image: initial; border-bottom-style: initial; border-bottom-color: initial; text-align: left; margin: 0px; padding: 6px 13px; }
table tr td { border: 1px solid rgb(223, 226, 229); text-align: left; margin: 0px; padding: 6px 13px; }
table tr th:first-child, table tr td:first-child { margin-top: 0px; }
table tr th:last-child, table tr td:last-child { margin-bottom: 0px; }
.CodeMirror-lines { padding-left: 4px; }
.code-tooltip { box-shadow: rgba(0, 28, 36, 0.3) 0px 1px 1px 0px; border-top: 1px solid rgb(238, 242, 242); }
.md-fences, code, tt { border: 1px solid rgb(231, 234, 237); background-color: rgb(248, 248, 248); border-radius: 3px; padding: 2px 4px 0px; font-size: 0.9em; }
code { background-color: rgb(243, 244, 244); padding: 0px 2px; }
.md-fences { margin-bottom: 15px; margin-top: 15px; padding-top: 8px; padding-bottom: 6px; }
.md-task-list-item > input { margin-left: -1.3em; }
@media print {
  html { font-size: 13px; }
  table, pre { break-inside: avoid; }
  pre { overflow-wrap: break-word; }
}
.md-fences { background-color: rgb(248, 248, 248); }
#write pre.md-meta-block { padding: 1rem; font-size: 85%; line-height: 1.45; background-color: rgb(247, 247, 247); border: 0px; border-radius: 3px; color: rgb(119, 119, 119); margin-top: 0px !important; }
.mathjax-block > .code-tooltip { bottom: 0.375rem; }
.md-mathjax-midline { background: rgb(250, 250, 250); }
#write > h3.md-focus::before { left: -1.5625rem; top: 0.375rem; }
#write > h4.md-focus::before { left: -1.5625rem; top: 0.285714rem; }
#write > h5.md-focus::before { left: -1.5625rem; top: 0.285714rem; }
#write > h6.md-focus::before { left: -1.5625rem; top: 0.285714rem; }
.md-image > .md-meta { border-radius: 3px; padding: 2px 0px 0px 4px; font-size: 0.9em; color: inherit; }
.md-tag { color: rgb(167, 167, 167); opacity: 1; }
.md-toc { margin-top: 20px; padding-bottom: 20px; }
.sidebar-tabs { border-bottom: none; }
#typora-quick-open { border: 1px solid rgb(221, 221, 221); background-color: rgb(248, 248, 248); }
#typora-quick-open-item { background-color: rgb(250, 250, 250); border-color: rgb(254, 254, 254) rgb(229, 229, 229) rgb(229, 229, 229) rgb(238, 238, 238); border-style: solid; border-width: 1px; }
.on-focus-mode blockquote { border-left-color: rgba(85, 85, 85, 0.12); }
header, .context-menu, .megamenu-content, footer { font-family: "Segoe UI", Arial, sans-serif; }
.file-node-content:hover .file-node-icon, .file-node-content:hover .file-node-open-state { visibility: visible; }
.mac-seamless-mode #typora-sidebar { background-color: var(--side-bar-bg-color); }
.md-lang { color: rgb(180, 101, 77); }
.html-for-mac .context-menu { --item-hover-bg-color: #E6F0FE; }
#md-notification .btn { border: 0px; }
.dropdown-menu .divider { border-color: rgb(229, 229, 229); }
.ty-preferences .window-content { background-color: rgb(250, 250, 250); }
.ty-preferences .nav-group-item.active { color: white; background: rgb(153, 153, 153); }

 .typora-export li, .typora-export p, .typora-export,  .footnote-line {white-space: normal;} 
</style>
</head>
<body class='typora-export os-windows' >
<div  id='write'  class = 'is-node'><h1><a name="machine-learning" class="md-header-anchor"></a><span>Machine Learning</span></h1><p><span>The goal of this document is to give an introduction to the general concepts and theory around Machine Learning and its applications. </span></p><p><span> </span></p><hr /><h3><a name="table-of-content" class="md-header-anchor"></a><span>Table Of Content</span></h3><ol start='' ><li><p><a href='#'><span>Introduction</span></a></p><ol start='' ><li><a href='#'><span>Understanding the nomenclature</span></a></li><li><a href='#'><span>Branches of Machine Learning</span></a></li></ol></li><li><p><a href='#'><span>ML Algorithms and Components</span></a></p><ol start='' ><li><a href='#'><span>Algorithms and parameters</span></a></li><li><a href='#'><span>Loss Function and Error</span></a></li><li><a href='#'><span>Training, validation and test</span></a></li><li><a href='#'><span>Evaluation metrics</span></a></li></ol></li><li><p><a href='#'><span>Neural Networks</span></a></p><ol start='' ><li><a href='#'><span>Perceptron</span></a></li><li><a href='#'><span>Neural Network</span></a></li></ol></li><li><p><a href='#'><span>Convolutional Neural Networks</span></a></p><ol start='' ><li><a href='#'><span>Convolutional filters</span></a></li><li><a href='#'><span>Convolutional layers</span></a></li><li><a href='#'><span>Pooling layers</span></a></li><li><a href='#'><span>Dense layers</span></a></li><li><a href='#'><span>Deep learning</span></a></li><li><a href='#'><span>Transfer learning</span></a></li></ol></li><li><p><a href='#'><span>Deep Learning Example</span></a></p></li><li><p><a href='#'><span>Typical Procedure</span></a></p></li></ol><hr /><p><span> </span></p><h2><a name="1.-introduction-<a-name=&quot;introduction&quot;></a>" class="md-header-anchor"></a><span>1. Introduction </span><a name="introduction"></a></h2><h3><a name="understanding-the-nomenclature-<a-name=&quot;nomenclature&quot;></a>" class="md-header-anchor"></a><span>Understanding the nomenclature </span><a name="nomenclature"></a></h3><p align="center"><img src="../assets/machineLearning/categories.png" width="400px" onerror="this.style.display = 'none';"></p><p><span> </span><strong><span>Artificial intelligence</span></strong><span> is used to describe machines that mimic &quot;cognitive&quot; functions that humans associate with the human mind, such as &quot;learning&quot; and &quot;problem solving&quot; [1]. This general domain contains image processing, cognitive science, machine learning, neural networks and much more. </span><strong><span>Machine learning</span></strong><span> on its end is a more specific subject of AI that nowadays encapsulates almost all AI research topics. Its core idea is that the computer does not just use a pre-written algorithm, but learns how to solve the problem itself. </span></p><p>&nbsp;</p><blockquote><p><em><span>Machine Learning is a field of study that gives computers the ability to learn without being explicitly programmed.</span></em><span> - </span><strong><span>Arthur Samuel</span></strong><span> (1959)</span></p></blockquote><p><span>Machine learning started flourishing with the arrival of the big data, the increase in computation power (GPU) and the development of new machine learning algorithms. </span></p><h4><a name="traditional-programming-vs-machine-learning" class="md-header-anchor"></a><span>Traditional programming VS Machine Learning</span></h4><p align="center"><img src="../assets/machineLearning/traditional.png" width="500px" onerror="this.style.display = 'none';"></p><p><span>In traditional programming you hard code the behavior of the program. In machine learning, you leave a lot of that to the machine to learn from the data iteratively. ML is used in the case when traditional programming strategy falls behind and it is not enough to fully implement a certain task. This is usually the case when the amount of inputs is too high, as with forecasts, image processing, speech recognition, etc.</span></p><h3><a name="branches-of-machine-learning-<a-name=&quot;branches&quot;></a>" class="md-header-anchor"></a><span>Branches of Machine Learning </span><a name="branches"></a></h3><p align="center"><img src="../assets/machineLearning/MLtypes.PNG" width="800px" onerror="this.style.display = 'none';"></p><p><strong><span>Reinforcement learning</span></strong><span> is about optimizing a decision making policy with experiences and rewards. It focuses on finding a balance between exploration of territory and exploitation of current knowledge. </span></p><p><em><a href='https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html'><span>Example</span></a><span>: the agent has to decide between two actions - moving the cart left or right - so that the pole attached to it stays upright</span></em></p><p align="center"><img src="../assets/machineLearning/RLcartpole.gif" width="400px" onerror="this.style.display = 'none';"></p><p><strong><span>Supervised learning</span></strong><span> tries to learn a function that maps an input to an output based on a learning process over training examples. In supervised learning, each example is a </span><em><span>pair</span></em><span> consisting of an input object and a desired output value (labels). This topic divides into classification (hottest subject in machine learning) and regression (forecasts, predictions). </span></p><p align="center"><img src="../assets/machineLearning/class_regress.PNG" width="500px" onerror="this.style.display = 'none';"></p><p><strong><span>Unsupervised learning</span></strong><span> helps to find unknown patterns in an input dataset without pre-existing labels to regroup inputs into clusters, reduce the number of dimensions or simplify a vast input into a few principle components. The main methods used in unsupervised learning are principal component and cluster analysis. </span></p><p align="center"><img src="../assets/machineLearning/clustering.png" width="500px" onerror="this.style.display = 'none';"></p><p><span>In image classification, supervised learning will extract features from the input and learn to correctly link these features to the input label. Unsupervised learning with extract features from the inputs and only try to regroup them into </span><em><span>different</span></em><span> clusters.</span></p><p align="center"><img src="../assets/machineLearning/unsupervised.png" width="600px" onerror="this.style.display = 'none';"></p><hr /><p><span> </span></p><h2><a name="2.-ml-algorithms-and-components-<a-name=&quot;components&quot;></a>" class="md-header-anchor"></a><span>2. ML Algorithms and Components </span><a name="components"></a></h2><h3><a name="algorithms-and-parameters-<a-name=&quot;algorithms&quot;></a>" class="md-header-anchor"></a><span>Algorithms and parameters </span><a name="algorithms"></a></h3><p><span>There is a wide range of algorithms available and none of them works best for all problems. Sometimes called a predictor, the algorithm will usually learn to optimize a prediction function </span><code>h</code><span> with parameters </span><code>θ</code><span> to optimize during training (if </span><strong><span>parametric</span></strong><span>). Here is a visual list of the most popular ones:</span></p><p align="center"><img src="../assets/machineLearning/predictors.png" onerror="this.style.display = 'none';"></p><p><span>It is important to note that not all algorithms are parametric. Like the simple </span><em><span>k-nearest neighbors</span></em><span> algorithm that is intentiated with a chosen number of neighbors to look at in order to infer the value of a new input. In this case, </span><code>k</code><span> is called an hyperparameter. In machine learning, an </span><strong><span>hyperparameter</span></strong><span> is a parameter whose value is set before the learning process begins, while parameters are defined during training. In the coding process, hyperparameters are passed in as arguments to the constructor of the model class. </span></p><p><span>Some examples of hyperparameters include loss function, regularization, learning rate, number of leaves in a tree, number of hidden layers in a neural network, number of clusters in clustering techniques... </span></p><h4><a name="capacity" class="md-header-anchor"></a><span>Capacity</span></h4><p><span>An important property of a machine learning algorithm is its </span><strong><span>capacity</span></strong><span>. The capacity of a model describes how complex a relationship it can model, although the term is loosely defined and cannot really be quantified. Conceptually, capacity represents the number of functions that a machine learning model can select as a possible solution. A general rule is that the more parameters a model has, the higher is its capacity. A low capacity model faced with a complex task will tend to underfit (high training error). On the other end, a high capacity model applied to a simple task might overfit (low training error, but high validation and test error). A model will often include a regularization function that will increase the loss with the increase in complexity to limit overfitting. </span></p><p align="center"><img src="../assets/machineLearning/complexity.png" width="450px" onerror="this.style.display = 'none';"></p><h3><a name="loss-function-and-error-<a-name=&quot;error&quot;></a>" class="md-header-anchor"></a><span>Loss Function and Error </span><a name="error"></a></h3><p><span>The empiric error </span><code>R_emp</code><span> corresponds to the mean of the loss calculated at each point with a chosen loss function </span><code>L(y, ŷ)</code><span> (usually either absolute error </span><code>|y-ŷ|</code><span> or quadratic error in regression, or cross entropy in classification) where </span><code>ŷ</code><span> is our prediction given by our predictor </span><code>h</code><span> with parameters </span><code>θ</code><span>. The predictor in our case is the neural network, while the parameters correspond to the weigths of its hidden layers. These parameters are optimized during training. </span></p><p align="center"><img src="../assets/machineLearning/error.png" width="550px" onerror="this.style.display = 'none';"></p><p><span>We could go deep into machine learning components to better understand not only how they work but mainly how one can work </span><em><span>well</span></em><span> (which as been the source of development for new ML algorithms). The list can be exhausting and a little math-oriented, so we will leave that aside for now since we want to jump into deep learning for image analysis. </span></p><h3><a name="training,-validation-and-test-<a-name=&quot;training&quot;></a>" class="md-header-anchor"></a><span>Training, validation and test </span><a name="training"></a></h3><p><span>The </span><strong><span>training</span></strong><span> procedure involves providing an ML algorithm with training data to learn from. For each sample, the model gives a prediction, calculates its error and the gradient. It then update its parameters directly (or through back-propagation) following a gradient descent. We usually predict a few samples in batches before calculating the error and backpropagating. This is set with the </span><strong><span>batch size</span></strong><span> hyperparameter (typically 64, 128...). A small batch size will make the gradient more noisy and &quot;active&quot; while a high batch size might make the training not efficient. Once the model has seen all the data, we have completed what is called an </span><strong><span>epoch</span></strong><span> (or iteration). This procedure will usually have to be carried multiple times for succesful training, hence multiple epochs. </span></p><p align="center"><img src="../assets/machineLearning/epochs.jpg" width="300px" onerror="this.style.display = 'none';"></p><p><span>The dataset is always </span><strong><span>split</span></strong><span> into training and test (usually around 80% training ratio). The </span><strong><span>test</span></strong><span> set is put on hold for final testing. Meanwhile during the training, a part of the training set is used for </span><strong><span>validation</span></strong><span> in-between epochs to measure overfitting. Validation set is usually obtained by taking 20% of the training data (or through a cross-validation technique). </span></p><p align="center"><img src="../assets/machineLearning/holdout.png" width="450px" onerror="this.style.display = 'none';"></p><p><span>Training and validation errors give a </span><strong><span>biased</span></strong><span> approximation of the risk of the model, while the test error gives an approximation of the risk that is not biased. </span></p><h3><a name="evaluation-metrics-<a-name=&quot;metrics&quot;></a>" class="md-header-anchor"></a><span>Evaluation metrics </span><a name="metrics"></a></h3><p><span>To evaluate the performance of a model, different metrics are available depending on the type of the task. For regression problems, the metrics used are the popular R^2 and mean squared/absolute error. We will here look at a few different classification metrics: classification accuracy, area under ROC curve, confusion matrix and f1-score. </span></p><p><span>It is first useful to know the basic definitions for classification prediction results: </span></p><p align="center"><img src="../assets/machineLearning/metrics.png" width="400px" onerror="this.style.display = 'none';"></p><p><strong><span>Classification accuracy</span></strong><span> is simply the percentage of correct predictions. This really is only suitable when there is an equal amount of observations for each class and that all predictions are equally important. </span></p><p><strong><span>Area under ROC curve</span></strong><span> is a performance metric for binary classification problems that represents a model’s ability to discriminate between positive and negative classes. An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model as good as random. The ROC curve is created by plotting the true positive rate (</span><em><span>sensitivity</span></em><span>) against the false positive rate (</span><em><span>false-alarm</span></em><span> or 1-specificity) at various threshold settings. </span></p><p align="center"><img src="../assets/machineLearning/roc.jpg" width="450px" onerror="this.style.display = 'none';"></p><p><span>The </span><strong><span>confusion matrix</span></strong><span> is a nice presentation of the accuracy of a model over two or more classes. Prediction labels are on the X-axis while the actual outcome labels are on the Y-axis. Each cell contains the number of predictions (or percentage) made for a given confusion state. The diagonal represents correct predictions. The following confusion matrix was obtained for a hand written digit classification model over the MNIST dataset.</span></p><p align="center"><img src="../assets/machineLearning/confusion.png" width="300px" onerror="this.style.display = 'none';"></p><p><span>From the confusion matrix a few metrics can be extracted for each class such as precision and sensitivity. The </span><strong><span>precision</span></strong><span> gives the performance for positive predictions, which is calculated using a prediction column in a confusion matrix. The </span><strong><span>sensitivity</span></strong><span> (or recall) evaluates the performance of positive predictions for a ground-truth label. These two metrics are often used in conjunction. Depending on the problem, a weigthed average of precision and sensitivity, called the </span><strong><span>F1-score</span></strong><span>, may be prioritized. </span></p><p>&nbsp;</p><h2><a name="3.-neural-networks-<a-name=&quot;nn&quot;></a>" class="md-header-anchor"></a><span>3. Neural Networks </span><a name="NN"></a></h2><p><span>After all the subjects we presented, we will focus our attention on supervised learning for classification, which is clearly the subject of interest for data analysis in science. To address the problem of classification, we will discuss two popular machine learning algorithms (</span><em><span>predictors</span></em><span>) suited for the task : the &quot;simple&quot; neural networks and the deep learning approach (mainly CNN). We will first introduce ourselves to the topic of neural networks by looking at the (very limited) perceptron. </span></p><p><em><span>An interesting and exhausting list of all the popular machine learning algorithms and predictors is also available </span><a href='https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/'><span>here</span></a><span>.</span></em></p><h3><a name="single-layer-nn-(or-perceptron)-<a-name=&quot;perceptron&quot;></a>" class="md-header-anchor"></a><span>Single-layer NN (or Perceptron) </span><a name="perceptron"></a></h3><p><span>The simplest neural network format is called a Perceptron and consists of a single input layer connected to their corresponding weights. A weighted sum is then calculated and fed into a step function. This </span><em><span>linear binary classifier</span></em><span> can be used to say whether or not an input belongs to some specific class. We can generally say that a perceptron is a single-layer neural network or single-neuron NN. </span></p><p align="center"><img src="../assets/machineLearning/perceptron.png" width="450px" onerror="this.style.display = 'none';"></p><p><span>To see a </span><strong><span>coding example</span></strong><span> of a simple perceptron I recommend looking at the </span><a href='https://medium.com/@thomascountz/19-line-line-by-line-python-perceptron-b6f113b161f3'><span>code and explanation by Thomas Countz</span></a><span> :</span></p><pre spellcheck="false" class="md-fences md-end-block ty-contain-cm modeLoaded" lang="python" style="break-inside: unset;"><div class="CodeMirror cm-s-inner CodeMirror-wrap" lang="python"><div style="overflow: hidden; position: relative; width: 3px; height: 0px; top: 0px; left: 8.16406px;"><textarea autocorrect="off" autocapitalize="off" spellcheck="false" tabindex="0" style="position: absolute; bottom: -1em; padding: 0px; width: 1000px; height: 1em; outline: none;"></textarea></div><div class="CodeMirror-scrollbar-filler" cm-not-content="true"></div><div class="CodeMirror-gutter-filler" cm-not-content="true"></div><div class="CodeMirror-scroll" tabindex="-1"><div class="CodeMirror-sizer" style="margin-left: 0px; margin-bottom: 0px; border-right-width: 0px; padding-right: 0px; padding-bottom: 0px;"><div style="position: relative; top: 0px;"><div class="CodeMirror-lines" role="presentation"><div role="presentation" style="position: relative; outline: none;"><div class="CodeMirror-measure"><span><span>​</span>x</span></div><div class="CodeMirror-measure"></div><div style="position: relative; z-index: 1;"></div><div class="CodeMirror-code" role="presentation" style=""><div class="CodeMirror-activeline" style="position: relative;"><div class="CodeMirror-activeline-background CodeMirror-linebackground"></div><div class="CodeMirror-gutter-background CodeMirror-activeline-gutter" style="left: 0px; width: 0px;"></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-keyword">import</span> <span class="cm-variable">numpy</span> <span class="cm-keyword">as</span> <span class="cm-variable">np</span></span></pre></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span cm-text="">​</span></span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-keyword">class</span> <span class="cm-def">Perceptron</span>(<span class="cm-builtin">object</span>):</span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span cm-text="">​</span></span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> &nbsp; &nbsp;<span class="cm-keyword">def</span> <span class="cm-def">__init__</span>(<span class="cm-variable-2">self</span>, <span class="cm-variable">no_of_inputs</span>, <span class="cm-variable">threshold</span>=<span class="cm-number">100</span>, <span class="cm-variable">learning_rate</span>=<span class="cm-number">0.01</span>):</span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> &nbsp; &nbsp; &nbsp; &nbsp;<span class="cm-variable-2">self</span>.<span class="cm-property">threshold</span> = <span class="cm-variable">threshold</span></span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> &nbsp; &nbsp; &nbsp; &nbsp;<span class="cm-variable-2">self</span>.<span class="cm-property">learning_rate</span> = <span class="cm-variable">learning_rate</span></span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> &nbsp; &nbsp; &nbsp; &nbsp;<span class="cm-variable-2">self</span>.<span class="cm-property">weights</span> = <span class="cm-variable">np</span>.<span class="cm-property">zeros</span>(<span class="cm-variable">no_of_inputs</span> <span class="cm-operator">+</span> <span class="cm-number">1</span>)</span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> &nbsp; &nbsp;<span class="cm-keyword">def</span> <span class="cm-def">predict</span>(<span class="cm-variable-2">self</span>, <span class="cm-variable">inputs</span>):</span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> &nbsp; &nbsp; &nbsp; &nbsp;<span class="cm-variable">summation</span> = <span class="cm-variable">np</span>.<span class="cm-property">dot</span>(<span class="cm-variable">inputs</span>, <span class="cm-variable-2">self</span>.<span class="cm-property">weights</span>[<span class="cm-number">1</span>:]) <span class="cm-operator">+</span> <span class="cm-variable-2">self</span>.<span class="cm-property">weights</span>[<span class="cm-number">0</span>]</span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> &nbsp; &nbsp; &nbsp; &nbsp;<span class="cm-keyword">if</span> <span class="cm-variable">summation</span> <span class="cm-operator">&gt;</span> <span class="cm-number">0</span>:</span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span class="cm-variable cm-error">activation</span> = <span class="cm-number">1</span></span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> &nbsp; &nbsp; &nbsp; &nbsp;<span class="cm-keyword">else</span>:</span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span class="cm-variable cm-error">activation</span> = <span class="cm-number">0</span> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> &nbsp; &nbsp; &nbsp; &nbsp;<span class="cm-keyword">return</span> <span class="cm-variable">activation</span></span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span cm-text="">​</span></span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> &nbsp; &nbsp;<span class="cm-keyword">def</span> <span class="cm-def">train</span>(<span class="cm-variable-2">self</span>, <span class="cm-variable">training_inputs</span>, <span class="cm-variable">labels</span>):</span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> &nbsp; &nbsp; &nbsp; &nbsp;<span class="cm-keyword">for</span> <span class="cm-variable">_</span> <span class="cm-keyword">in</span> <span class="cm-builtin">range</span>(<span class="cm-variable-2">self</span>.<span class="cm-property">threshold</span>):</span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span class="cm-keyword">for</span> <span class="cm-variable">inputs</span>, <span class="cm-variable">label</span> <span class="cm-keyword">in</span> <span class="cm-builtin">zip</span>(<span class="cm-variable">training_inputs</span>, <span class="cm-variable">labels</span>):</span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span class="cm-variable">prediction</span> = <span class="cm-variable-2">self</span>.<span class="cm-property">predict</span>(<span class="cm-variable">inputs</span>)</span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span class="cm-variable-2">self</span>.<span class="cm-property">weights</span>[<span class="cm-number">1</span>:] += <span class="cm-variable-2">self</span>.<span class="cm-property">learning_rate</span> <span class="cm-operator">*</span> (<span class="cm-variable">label</span> <span class="cm-operator">-</span> <span class="cm-variable">prediction</span>) <span class="cm-operator">*</span> <span class="cm-variable">inputs</span></span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span class="cm-variable-2">self</span>.<span class="cm-property">weights</span>[<span class="cm-number">0</span>] += <span class="cm-variable-2">self</span>.<span class="cm-property">learning_rate</span> <span class="cm-operator">*</span> (<span class="cm-variable">label</span> <span class="cm-operator">-</span> <span class="cm-variable">prediction</span>)</span></pre><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span></pre></div></div></div></div></div><div style="position: absolute; height: 0px; width: 1px; border-bottom: 0px solid transparent; top: 540px;"></div><div class="CodeMirror-gutters" style="display: none; height: 540px;"></div></div></div></pre><h3><a name="neural-network-(multi-layer)--<a-name=&quot;multi&quot;></a>" class="md-header-anchor"></a><span>Neural Network (multi-layer)  </span><a name="multi"></a></h3><p align="center"><img src="../assets/machineLearning/neuralNetwork.jpeg" width="350px" onerror="this.style.display = 'none';"></p><p><span>A neural network usually differs from the perceptron by having at least one hidden layer. A neural network has </span><strong><span>nodes</span></strong><span> (neurons) and </span><strong><span>edges</span></strong><span> (connections). Each node and edge usually has an associated </span><strong><span>weight</span></strong><span> (parameter) that is tuned during learning process. The weight changes the strength of the signal at a connection. For each hidden node (a perceptron on its own), the output is calculated by some </span><strong><span>non-linear</span></strong><span> sum of its inputs, the </span><strong><span>activation function</span></strong><span> (Sigmoid, Tanh, ReLu). It is important to give non-linarity to the model so it can learn to represent more complex non-linear mappings between inputs and outputs. Sometimes, a </span><strong><span>bias</span></strong><span> parameter is added to the sum to serve as a threshold to shift the activation function. For classification problems, the output layer will have the same number of nodes than the number of different classes. A </span><strong><span>softmax</span></strong><span> function is usually applied at the output to obtain a vector of probabilities that sum to 1. </span></p><p align="center"><img src="../assets/machineLearning/softmax.png" width="250px" onerror="this.style.display = 'none';"></p><p><span>The weights are initially set randomly (following a desired distribution). During learning process, the weights get adjusted after calculation of the error through a method called backpropagation. </span></p><p><span>To see a </span><strong><span>coding example</span></strong><span> of a neural network in detail (including backpropagation), I recommend looking at the </span><a href='https://enlight.nyc/projects/neural-network/'><span>code and explanation by Samay Shamdasani</span></a></p><hr /><p><span> </span></p><h2><a name="4.-convolutional-neural-networks-(cnn)-<a-name=&quot;cnn&quot;></a>" class="md-header-anchor"></a><span>4. Convolutional Neural Networks (CNN) </span><a name="CNN"></a></h2><h3><a name="convolutional-filters<a-name=&quot;filters&quot;></a>" class="md-header-anchor"></a><span>Convolutional filters</span><a name="filters"></a></h3><p><span>As the name suggests, a basic CNN still uses a somewhat similar architecture as the neural network we just studied, even though CNNs usually go into deep learning which means new algorithms and more complex architectures. The main difference are the hidden layers: instead of having hundreds of nodes fully connected together, we use a &#39;few&#39; convolutional </span><strong><span>filters</span></strong><span> that are all moved accross the input image to create their own representation. The output of a filter usually has the same dimension as its input, compared to a NN node which always returns a scalar. Following this, a 2D filter 3x3 will have 9 weights (parameters) to adjust during training, but since it is not fully connected to all pixels, this generates a lot less parameters in the end, compared to neural networks. </span></p><p align="center"><img src="../assets/machineLearning/convolution.png" width="450px" onerror="this.style.display = 'none';"></p><p><span>A convolutional filter always has the same depth as the input tensor. The filter is moved accross the image and the output value is calculated at each step. Here is a visualisation for a filter of size 5x5 over an RGB image: </span></p><p align="center"><img src="../assets/machineLearning/singlefilter.png" width="250px" onerror="this.style.display = 'none';"></p><p><span>To keep the same dimension at output, a </span><strong><span>zero-padding</span></strong><span> can be added to the input image. To reduce dimension, we can tell the filter to step aside 2 pixels (</span><strong><span>stride</span></strong><span> of 2) at a time or more before computing the convolution. </span></p><figure><table><thead><tr><th style='text-align:center;' ><span>Padding = 1</span></th><th style='text-align:center;' ><span>Stride = 2</span></th></tr></thead><tbody><tr><td style='text-align:center;' ><img src="../assets/machineLearning/padding.gif" width="250px"/></td><td style='text-align:center;' ><img src="../assets/machineLearning/stride.gif" width="250px"/></td></tr></tbody></table></figure><p>&nbsp;</p><h3><a name="convolutional-layers-<a-name=&quot;layer&quot;></a>" class="md-header-anchor"></a><span>Convolutional layers </span><a name="layer"></a></h3><p><span>A convolution layer is simply defined by having multiple filters that will move accross the input to each generate their own feature map representation. A convolutional layer will have a few hyperparameters such as the number of filters, their shape, their initial state, their stride (pixel stepping: usually 1), zero-padding.</span></p><p align="center"><img src="../assets/machineLearning/featuremaps.png" width="400px" onerror="this.style.display = 'none';"></p><p><span>This process will be done again a few times (number of convolutional layers) in deep learning. </span></p><p align="center"><img src="../assets/machineLearning/secondfilter.png" width="300px" onerror="this.style.display = 'none';"></p><p><span>This is the main idea behind deep learning...</span></p><p align="center"><img src="../assets/machineLearning/baseidea.png" width="400px" onerror="this.style.display = 'none';"></p><h3><a name="pooling-layers<a-name=&quot;pooling&quot;></a>" class="md-header-anchor"></a><span>Pooling layers</span><a name="pooling"></a></h3><p><span>Since convolution will keep the input shape (except for the contour), this usually leads to an excess in dimensionality. All CNNs fix this by adding intermediate max pooling (or average pooling) layers to down-sample the input representation. </span></p><p align="center"><img src="../assets/machineLearning/maxpooling.png" width="450px" onerror="this.style.display = 'none';"></p><h3><a name="dense-layers<a-name=&quot;dense&quot;></a>" class="md-header-anchor"></a><span>Dense layers</span><a name="dense"></a></h3><p><span>A dense layer, or fully-connected layer, is just a regular layer of neurons in a neural network, as we just looked at. They are usually inserted at the end of a CNN to classify. We usually say that the convolutional layers act as a feature extractor, while the dense layers act as the classifier. Notice that the output of the following </span><em><span>AlexNet</span></em><span> CNN is a vector of length 1000 which means his network was used to classify images into 1000 different classes (bus, train, person, dog, etc.). </span></p><h3><a name="deep-learning-<a-name=&quot;deep&quot;></a>" class="md-header-anchor"></a><span>Deep learning </span><a name="deep"></a></h3><p><span>This leads to an architecture that looks like this for the popular </span><strong><em><span>AlexNet</span></em></strong><span> CNN with 5 convolutional layers and 3 dense layers: </span></p><p align="center"><img src="../assets/machineLearning/alexNet.png" width="800px" onerror="this.style.display = 'none';"></p><p><em><span>This image hides the other half of </span><a href='https://iq.opengenus.org/architecture-and-use-of-alexnet/'><span>AlexNet</span></a><span> used to run on 2 GPUs at the same time.</span></em></p><p><span>In deep learning, each feature layer usually reaches a deeper level of abstraction or complexity, from simple edges to actual objects. This is refered to </span><strong><span>feature hierarchy</span></strong><span>, and is better visualized:</span></p><p align="center"><img src="../assets/machineLearning/hierarchy.png" width="500px" onerror="this.style.display = 'none';"></p><h3><a name="transfer-learning-<a-name=&quot;transfer&quot;></a>" class="md-header-anchor"></a><span>Transfer Learning </span><a name="transfer"></a></h3><p><span>Transfer learning is a machine learning method where a model developed for a task is reused as the starting point for a model on a second task. This is popular in deep learning given the resources and datasets required to train the models. The transfer learning only works if the features learned from the first task (base) are general enough to be applicable to the second task (target). </span></p><p><span>In transfer learning, the first step is to find a pre-trained model whose training data is somewhat similar to yours. Unless you have the same task, the model will surely need some tuning. This might only be about resetting the dense layers (classifier) with the appropriate number of classes. </span></p><p align="center"><img src="../assets/machineLearning/transfer_learning.jpg" width="500px" onerror="this.style.display = 'none';"></p><p><span>The convolutional layers are then freezed during training where you will be optimizing the dense layers weights to better suit your task. Depending on the (non-)similarity of your data to the pre-trained model&#39;s data, you might have to unfreeze a few convolutional layers at the head of the model. The more layers you tune, the more data you&#39;ll need, but the more accurate (and specific) it should become. </span></p><p><span>For image data, it is common to reuse a model trained for the ImageNet classification competition. These models would take days or weeks to train on modern hardware. </span></p><p>&nbsp;</p><hr /><h2><a name="deep-learning-coding-tutorial<a-name=&quot;example&quot;></a>" class="md-header-anchor"></a><span>Deep Learning Coding Tutorial</span><a name="example"></a></h2><p><span>To see a </span><strong><span>coding example</span></strong><span> on deep learning in detail, I recommend looking at the </span><a href='https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a'><span>tutorial by</span>
<span>Dipanjan Sarkar</span></a><span> or at its source code that I executed and commented on a </span><a href='https://github.com/DCC-Lab/Documentation/blob/master/assets/machineLearning/examples/catdogML.ipynb'><strong><span>jupyter notebook ML example</span></strong></a><span>. </span></p><p align="center"><img src="../assets/machineLearning/examples/catsDogs.gif" width="600px" onerror="this.style.display = 'none';"></p><p>&nbsp;</p><hr /><h2><a name="typical-procedure<a-name=&quot;procedure&quot;></a>" class="md-header-anchor"></a><span>Typical procedure</span><a name="procedure"></a></h2><ul><li><p><span>Load data and labels</span></p><blockquote><p><span>Might have to resize images, apply filters...</span>
<span>If its too big to load, think about writing a DataGenerator class.</span></p></blockquote></li><li><p><span>Split dataset into training, validation and test</span></p><blockquote><p><span>Usually around 60% training, 20% validation, 20% test.</span>
<span>Make sure the data is different for each set</span></p></blockquote></li><li><p><span>Scale</span></p><blockquote><p><span>Simple normalization (remove minimum, divide by max value)</span>
<span>Or StandardScale (unit-variance and zero-mean)</span></p></blockquote></li><li><p><span>Build the model</span></p><blockquote><p><span>Look at other models online, recycle, transfer learning if possible. </span>
<span>Compile with an appropriate loss function and optimizer for your problem. </span></p></blockquote></li><li><p><span>Train</span></p><blockquote><p><span>Batch size usually around 32, 64, 128.</span>
<span>Use callbacks for early stopping, checkpoints. Then nb. of epochs can be anything high enough...</span></p></blockquote></li><li><p><span>Evaluate on test set</span></p></li></ul><p><a href='https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Keras_Cheat_Sheet_Python.pdf'><strong><span>Keras Coding CheatSheet</span></strong></a></p><p>&nbsp;</p><hr /><p>&nbsp;</p><h4><a name="references" class="md-header-anchor"></a><span>References</span></h4><p><span>[1] </span><a href='https://en.wikipedia.org/wiki/Stuart_J._Russell'><span>Russell, Stuart J.</span></a><span>; </span><a href='https://en.wikipedia.org/wiki/Peter_Norvig'><span>Norvig, Peter</span></a><span> (2009). </span><em><span>Artificial Intelligence: A Modern Approach</span></em><span> (3rd ed.).</span>
<span>[2] École d&#39;hiver en apprentissage automatique (2019), Université Laval.</span></p><p>&nbsp;</p></div>
</body>
</html>